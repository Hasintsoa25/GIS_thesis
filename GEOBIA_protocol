
// Nombre total de segments : 58903 si size = 10
////////////// ====================== Comparaison entre l'approche GEOBIA et methode de classif simple===============================================////////////////
/////////////                     ADOPTEEE SUR L'ECOSYSTEME NATUREL pour le cas de la region BOENY DE MADAGASCAR


// 1-----  Importation des donn√©e brutes avec les indices spectrales et la delimitation de la zone d'etude
var survey = antrema_2024  // images brutes multibandes (indices)
var geom = survey.geometry() // delimitation de la zone d'√©tude
var scale = 10  // echelle de calcul de tratement // dimension d'un pixel

//1----------a Calcul des bandes
var bsi = survey.expression(
  '((SWIR + RED) - (NIR + BLUE)) / ((SWIR + RED) + (NIR + BLUE))', {
    'SWIR': survey.select('B11'),
    'RED':  survey.select('B4'),
    'NIR':  survey.select('B8'),
    'BLUE': survey.select('B2')
  }).rename('BSI');

var survey = survey.addBands(bsi)

// Visualtion de S
var s1 = ee.ImageCollection('COPERNICUS/S1_GRD')
  .filterBounds(geom)
  .filterDate('2024-01-01', '2025-12-31')
  .filter(ee.Filter.eq('instrumentMode','IW'))
  .filter(ee.Filter.eq('orbitProperties_pass','DESCENDING'))
  .filter(ee.Filter.listContains('transmitterReceiverPolarisation','VV'))
  .filter(ee.Filter.listContains('transmitterReceiverPolarisation','VH'))

  
var s1_mean = s1.select(["VV","VH"]).mean()
// Calcul du ratio VH/VV (plus l√©ger ici)
var ratio = s1_mean.select('VH').divide(s1_mean.select('VV')).rename('ratio')
          .resample("bilinear");

// Image finale Sentinel-1

//on a55 images de s1
              
//======================================================//
//             INTEGRATION DE S1 dans S2

var survey = survey.addBands(ratio)
print("all variable bands : ",survey.bandNames())


// =============================== MIS EN APPLICATION DE SIMPLE NON ITERATICE CLUSTERING  ==========================================

// selon Zhang, D., Ying, C., Wu, L., Meng, Z., Wang, X., & Ma, Y. (2023). Using Time Series Sentinel Images for Object-Oriented Crop Extraction of Planting Structure in the Google Earth Engine. Agronomy, 13(9), 2350. https://doi.org/10.3390/agronomy13092350 
//2 ----- Define the superpixel seed location spacing, in pixels: (5 - 10 - 15 - 20)
var size_segmentation = 10 //units pixels/  15 is the value by defaults, by i can make it 10// Enfonction de l'espacement de chaque couverture de sols

//3----- Define the GLCM indices used in input for the PCA
var glcm_bands= ["gray_asm","gray_contrast","gray_corr","gray_ent","gray_var","gray_idm","gray_savg"]// glcm bands

//4-----Object-based Approach ( SUPERPIXELS)

// Segmentation using a SNIC approach based on the dataset previosly generated
var seeds = ee.Algorithms.Image.Segmentation.seedGrid(size_segmentation);

//set the bands names to snic computing
var snic_bands = [
                  "NDVI",//"NDVI_STD",
                  "MNDWI",//"NDWI",//"NDWI_STD",
                  "SAVI_MAX","BSI",
                  "ratio"
                  ]
                  
// Apres la resulatat de la calssification OA, lorce que j'ai ajout√©
//B8 et B4 on a oa 0.86
//Mais si je l enleve ces deux bandes, et j'ajoute ratio
//j'ai
var snic = ee.Algorithms.Image.Segmentation.SNIC({
  image: survey.select(snic_bands), 
  compactness:0.1,  
  connectivity: 8, // approche de la segmenetation , options 4 ou 8
  neighborhoodSize: 256, 
  seeds: seeds //15
})
//Create and rescale a grayscale image for GLCM ( Enfin de compte c'est une ponderation de bandes pour avoir une image en fonction de ces bandes)
//Pour mon experience donc, je vais choisir d'utiliser soir EVI ,soit SAVI , ces indices prenneen ten compte des corrections au niveaux des aerosol
// atmo et la reflectance du sol


// 5 --------- the glcmTexture size (in pixel) can be adjusted considering the spatial resolution and the object textural characteristics
var gray = survey.select('NDVI').rename('gray').unitScale(-1,1).multiply(255).uint8() // j'ai pris NDVI (bandes utilses pour diiferencier les LUCL)
var glcm = gray.glcmTexture({size: 2});

/// Set up the min and max value of the gray band image
var minMax = gray.reduceRegion({
  reducer:ee.Reducer.minMax(), 
  geometry:geom, 
  scale:10,
  bestEffort : true
})
 var gray_min = minMax.get('gray_min') //65 NDVI
 var gray_max = minMax.get('gray_max') //226
//print("La valeur maximale de la bande gray : ",ee.Number(gray_max),"  la valeur doit s'etaler de 0-255")
//print("La valeur minimale de la bande gray : ",ee.Number(gray_min),"Plus la valeur s'etire plus, l'algorithme glcm est bon")


//----- Define the GLCM indices used in input for the PCA
var glcm_bands= ["gray_asm","gray_contrast","gray_corr","gray_ent","gray_var","gray_idm","gray_savg"]

//print("Liste de bandes issues des resultats obtenus glcm : ",glcm.bandNames())
//////////////////////////////////////////PCA METHODS//////////////////////////////////////
//--- Before the PCA the glcm bands are scaled
var image = glcm.select(glcm_bands);

// ====================================  PCA APPROACH  ===============================

// calculate the min and max value of an image
var minMax = image.reduceRegion({
  reducer: ee.Reducer.minMax(),
  geometry: geom,
  scale: scale,
  maxPixels: 1e16,
  bestEffort:true
});

// Normalize the data
var glcm = ee.ImageCollection.fromImages(
  image.bandNames().map(function(name){
    name = ee.String(name);
    var band = image.select(name);
    return band.unitScale(ee.Number(minMax.get(name.cat('_min'))), ee.Number(minMax.get(name.cat('_max'))))
})).toBands().rename(image.bandNames());

//---- Apply the PCA
// The code relating to the PCA was adapted from the GEE documentation  https://developers.google.com/earth-engine/guides/arrays_eigen_analysis

// Get some information about the input to be used later.


var bandNames = glcm.bandNames();

// Mean center the data to enable a faster covariance reducer and an SD stretch of the principal components.
var meanDict = glcm.reduceRegion({
    reducer: ee.Reducer.mean(),
    geometry: geom, 
    scale: scale,
    maxPixels: 1e16,
    bestEffort:true
});

//,On doit transofmer sous forme une image constante les valeurs moyenn√©es pur chaque bands
//Donc::::
var means = ee.Image.constant(meanDict.values(bandNames))
//soustraction par la moyenne de l 'image constante (band math)
var centered = glcm.subtract(means);


// This helper function returns a list of new band names.
var getNewBandNames = function(prefix) {
  var seq = ee.List.sequence(1, bandNames.length());
  return seq.map(function(b) {
    return ee.String(prefix).cat(ee.Number(b).int());
  });
};

// This function accepts mean centered imagery, a scale and a region in which to perform the analysis. 
// It returns the Principal Components (PC) in the region as a new image.
var getPrincipalComponents = function(centered, scale) {
  // Collapse the bands of the image into a 1D array per pixel.
  var arrays = centered.toArray();
  
  // Compute the covariance of the bands within the region.
  var covar = arrays.reduceRegion({
    reducer: ee.Reducer.centeredCovariance(),
    geometry: geom,
    scale: scale,
    bestEffort:true,
    maxPixels:1e13
  });
  
  // Get the 'array' covariance result and cast to an array.
  // This represents the band-to-band covariance within the region.
  var covarArray = ee.Array(covar.get('array'));
  
  // Perform an eigen analysis and slice apart the values and vectors.
  var eigens = covarArray.eigen();
  
  // This is a P-length vector of Eigenvalues.
  var eigenValues = eigens.slice(1, 0, 1);
  // This is a PxP matrix with eigenvectors in rows.
  var eigenVectors = eigens.slice(1, 1);
    
  // Convert the array image to 2D arrays for matrix computations.
  var arrayImage = arrays.toArray(1);
    
  // Left multiply the image array by the matrix of eigenvectors.
  var principalComponents = ee.Image(eigenVectors).matrixMultiply(arrayImage);
    
  // Turn the square roots of the Eigenvalues into a P-band image.
  var sdImage = ee.Image(eigenValues.sqrt())
    .arrayProject([0]).arrayFlatten([getNewBandNames('sd')]);
  
  // Turn the PCs into a P-band image, normalized by SD.
  return principalComponents
    // Throw out an an unneeded dimension, [[]] -> [].
    .arrayProject([0])
    // Make the one band array image a multi-band image, [] -> image.
    .arrayFlatten([getNewBandNames('pc')])
    // Normalize the PCs by their SDs.
    .divide(sdImage);
};

// Get the PCs at the specified scale and in the specified region
var pcImage = getPrincipalComponents(centered, scale, geom);


//===============================PCA PROCESS ENDING ==============================================

//

//Select the band "clusters" from the snic output fixed on its scale of 10 meters and add them the PC1 taken from the PCA data.
// Calculate the mean for each segment with respect to the pixels in that cluster
var clusters_snic = snic.select("clusters")

// ========================================================================
//                    WITH GLCM
// ========================================================================

var pca_added = clusters_snic.addBands(pcImage.select("pc1"))

var pca_mean_clustering = pca_added.reduceConnectedComponents({
  reducer: ee.Reducer.mean(),
  labelBand: 'clusters'
})

//Create a dataset with the new band used so far together with the band "clusters" and their new mean parameters
var final_bands = pca_mean_clustering.addBands(snic)
//Map.addLayer(final_bands,{},'Visualisation')
//print("final_bands : ",final_bands.bandNames())
/////////////////////////////////////////CLASSIFICATION GEOBIA APPROACH///////////////////////////////////


//a -----------Sampling data dor the classification
//Creation of the "newfc" feature collection using the pixels having a feature property called "LULC" 
var sampling = water.merge(wetlands).merge(tanne).merge(mi).merge(md).merge(fi)
                    .merge(fd).merge(sa).merge(sol_nu)

//b -----------To improve the information using a buffer with a fixed radius for the natural classification  ( radius = 10 m)


var sampling_geobia = sampling
var sampling_nogeobia =  sampling.map(function(feature){
  return feature.buffer(10)
})


// c -----------Nommer les nomenclatures
var names = ["water","wetlands","tanne","mi","md","fi","fd","sa","sol_nu"];


//d -----------Define the training bands removing just the "clusters" bands
var geobia_bands = final_bands.bandNames().remove("clusters")
//print("Bands with GLCM PCA : ",geobia_bands) // 6 bands 

// == pour information
var clusters = final_bands.select("clusters").reduceRegion({
  reducer: ee.Reducer.countDistinct(), 
  geometry:geom, 
  scale: scale
  
})
//print("nombre total de segments: ",clusters)


// Pout l'utilisation de svm, on doit renormaliser les donn√©es issues des bandes


                   // =========================================================================// 

var minMax = final_bands.reduceRegion({
  reducer: ee.Reducer.minMax(),
  geometry: geom,
  scale: 10,
  maxPixels:10e13,
  bestEffort:true
}); 
// use unit scale to normalize the pixel values
var data_norm = ee.ImageCollection.fromImages(
  final_bands.bandNames().map(function(name){
    name = ee.String(name);
    var band = final_bands.select(name);
    return band.unitScale(ee.Number(minMax.get(name.cat('_min'))), ee.Number(minMax.get(name.cat('_max'))))
})).toBands().rename(final_bands.bandNames());
                  // =========================================================================//
// ========================================================================
//Classification using the classifier with the training bands called geobia_bands
//e ----------- Training with sampling data

var training_GEOBIA = final_bands.select(geobia_bands).sampleRegions({
  collection: sampling_geobia,
  properties: ['LULC'],
  scale: 10})
//print(training_GEOBIA)
// Splitting options
var split_option = 0.7; // 70% training, 30% validation
// ========================================================================
//                    WITHOUT GEOBIA
// ========================================================================
//////////// Second method is to compare our model with GLCM methout , without it/////

var bands = ["NDVI","MNDWI","SAVI_MAX","ratio","BSI"]
var training_withoutGEOBIA = survey.select(bands).sampleRegions({
  collection : sampling_nogeobia,
  properties : ["LULC"], 
  scale : scale
  
})

var ranW = training_withoutGEOBIA.randomColumn({
  columnName : "aleas", 
  distribution : "uniform",
  seed:42
  
})// 1452 samples

var training_sampleW = ranW.filter(ee.Filter.lte('aleas', split_option)); //1170

var testing_sampleW= ranW.filter(ee.Filter.gte('aleas', split_option)); //282

///////////////////////////////////////////////////////
///// PPourt le cas de without GLCM, je crois que je vais faire juste une simple classification, mais j ai pas envie de comparer
// l'utilisation de oui ou non, peut etre a autre jour, pourquoiii???
// parce que la methode laplus utilis√©√© aujourd hui c'est juste faire des zones d'apprentissage sans faire des pre traitements faits precedemment

// ========================================================================

// ========================================================================
// Exportation du fichier excell pour le traitement vers python (vscode) 
/*
Export.table.toDrive({collection :  training_GEOBIA, 
                      description : "sampling_2024", 
                      folder:"DataBase_Antrema",
                      fileFormat:"SHP"
  
})
*/
// ========================================================================


///////////////////////////////////////////////////////////////////////////////////////////////////////
// 
//================================= CREATION D'UN LOOPING EN INTEGRANT RF et SVM, et les deux types d'apprentissange GEOBIA et non ============================================

//f ----------- Split the data into training and validation sets

var ran = training_GEOBIA.randomColumn({
  columnName : "aleas", 
  distribution : "uniform",
  //rowKeys:["LULC"],
  seed:0
  
})// 1452 samples

var training_sample = ran.filter(ee.Filter.lte('aleas', split_option)); //1170
var testing_sample= ran.filter(ee.Filter.gte('aleas', split_option)); //282

//g -----------Training the classifier
// Pourquoi RF est le meiileur modeele, on doit verifier pour tous les reguions non?????
// Utilisation des modeles svm et rf, si on a la preuve dans la region boeny que rf est la meilleure dans toutes les identifications
// des ecosyst√®mes foreties naturels

// ================ D√©finition des classifieurs √† tester ==============================

///////////////////                  RF vs SVM        ////////////////////////
var algos = [ /// creation d'un dictionnaire comprenant les types d'algorithmes
  {
    algo_name: 'Random_Forest',  // colonne nomm√© nom
    algo_classifier: ee.Classifier.smileRandomForest({ // dexueme cl√© nomm√© classifier
      numberOfTrees: 90,
      seed:142
    })
  },
  {
    algo_name: 'SVM_RBF',
    algo_classifier: ee.Classifier.libsvm({
      kernelType: 'RBF',
      cost: 10,
      gamma: 0.5
    })
  }
];

// =============== Boucle sur les classifieurs

algos.forEach(function(modeles) {
  // Entra√Ænement
  var trained = modeles.algo_classifier.train({
    features: training_sample,
    classProperty: 'LULC',
    inputProperties: geobia_bands
  });
  
  // Pr√©diction
  var classified = final_bands.select(geobia_bands).classify(trained);
  
  // Validation
  var validation = testing_sample.classify(trained);
  var confMatrix = validation.errorMatrix('LULC', 'classification');
  var overallAcc = confMatrix.accuracy();
  var pa = confMatrix.producersAccuracy();
  var ua = confMatrix.consumersAccuracy();
  var f_score = confMatrix.fscore()

  // Affichage des r√©sultats
  print('üìä R√©sultats pour GOEBIA ' + modeles.algo_name);
  print("Les bandes les plus importantes dans la classification selon: " + modeles.algo_name)
  print(trained.explain());
  print('Matrice de confusion:', confMatrix);
  print('Accuracy globale: ', overallAcc);
  print('Producer accuracy / indice de commsion : ', pa);// combien de la classe reelle a √©t√© bien predite
  print("User's accuracy  / indice d'omission: ",ua);//combien de la classe dite predite appartient classe reelle
  print('F1-score par classe:', f_score);
    
    
    //Map.addLayer(classified,vizClassif,"classification obtenue par "+ modeles.algo_name)
        // Exportation des images classifi√©es
    /*
    Export.image.toDrive({
      image:classified, 
      description : "Classify_" + modeles.algo_name + "_2025", 
      folder : "Classif_GEOBIA", 
      fileNamePrefix : modeles.algo_name,
      region:geom, 
      scale:scale,
      crs:"EPSG:4326"
      
    })
*/
})
// Pour le cas de SVM : kappa : 0.81, pour RF : 0.95
// =====================================================================


///////////////////////// deuxieme cas, classification simple
algos.forEach(function(modelesW) {
  // Entra√Ænement
  var trainedW = modelesW.algo_classifier.train({
    features: training_sampleW,
    classProperty: 'LULC',
    inputProperties: bands
  });
  
  // Pr√©diction
  var classified = final_bands.select(bands).classify(trainedW);
  
  // Validation
  var validation = testing_sampleW.classify(trainedW);
  var confMatrix = validation.errorMatrix('LULC', 'classification');
  var overallAcc = confMatrix.accuracy();
  var ua = confMatrix.consumersAccuracy()
  var pa = confMatrix.producersAccuracy()
  var f1_score =  confMatrix.fscore()
  
  // Affichage des r√©sultats
  print('üìä R√©sultats pour NON GOEBIA ' + modelesW.algo_name);
  print("Les bandes les plus importantes dans la classification selon: " + modelesW.algo_name)
  print(trainedW.explain());
  print('Matrice de confusion:', confMatrix);
  print('Accuracy globale: ', overallAcc);
  print('Producer accuracy / indice de commsion : ', pa);// combien de la classe reelle a √©t√© bien predite
  print("User's accuracy  / indice d'omission: ",ua);//combien de la classe dite predite appartient classe reelle
  print('F1-score par classe:', f1_score);
    

})


///////////////////////////////////////////////////////////////////////////////////////////////////////
// Si on a fait une taille de fragmentation de : 
// 10  on a  : ovrall accuracy : 0.9619 et kappa : 0.95
// 15 on a : overall accuracy de : 0.978 et kappa :0.975 // plus c est grand plus la valeur augmente
// mais la taille de segmentation est grande et cela n'est pas adapt√© pour la description de l'zcosysteme de Antrema
// si les unit√©s pasysage√®re est grande , on peut choisir 15

 /////////////////////////////////////////////////////////////////////
 
// Ici on va analuysezr une methode avec GOEBIA et non GEOBIA,
// les zones d'apprentissage se fait par polygone mais non pas par segmentation fait pas SNIC
// Mais on garde neamnois les sampling deja utilis√©s lors GEOBIA, pour avoir une comparaison
//homog√®ne
//Les optimisation des param√®tres ont √©√©t d√©j√† fait via python (bandes  et n_tree)


/////////
//===================CLASSIFICATION SIMPLE using random forest and sampling delineation==============================
////////

//a-------------------  Mon id√©√© c'est de √† partir de ces points d'chantillonage j vasi creer les polygones d'entrainement ////////////////////////
// afin d'avoir une certaine homogeneisation


/*
///////////////////////SPLIT THE SCREENNN /////////////////////////
/// Split the image
// Create two map layers for the split map
var leftMap = ui.Map();
var rightMap = ui.Map();

// Add the first image to the left map
leftMap.addLayer(classy_RF,viz);
var landLabel = ui.Label('Random forest with glcm')
landLabel.style().set('position',"bottom-left")
leftMap.add(landLabel)
// Add the second image to the right map
rightMap.addLayer(classy_RFW,viz);
var S2label = ui.Label('Random forest without glcm')
S2label.style().set('position','bottom-right')
rightMap.add(S2label)

// Create a split panel to display the two maps side by side
var splitPanel = ui.SplitPanel({
  firstPanel: leftMap,
  secondPanel: rightMap,
  orientation: 'horizontal', // 'horizontal' or 'vertical'
  wipe: true// Enable the swipe tool
});
ui.root.clear()

ui.root.add(splitPanel)
var linkPanel = ui.Map.Linker([leftMap,rightMap])
*/
ui.root.add(splitPanel)
var linkPanel = ui.Map.Linker([leftMap,rightMap])
*/
